import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import seaborn
import pandas as pd
import matplotlib.pyplot as plt
import copy
import os
import argparse
import sys
import csv
from random import randrange
from model import PreferenceLearner

NUM_HIDDEN = 20

def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')
        
def get_preference(score_a, score_b):

    if score_a == score_b:
        return 0.5 # 0
    elif score_a > score_b:
        return 1
    else:
        return 0 # -1
        
        
def match_result_to_str(match_result):
    result = ""
    for i in range(0, len(match_result)):
        result = result + "{:.2f}".format(match_result[i]) + ","
    return result

   
parser = argparse.ArgumentParser(description="Arg parser")
parser.add_argument("-d", "--debug", type=str2bool, nargs='?', const=True, default=False, help="Activate debug print statements.")
parser.add_argument("-m", "--match_results", type=str, nargs='?', const=True, required=True, help="Path of match results file.")
parser.add_argument("-n", "--num_training_samples", type=int, nargs='?', const=True, required=True, help="Number of training samples to use.")
parser.add_argument("-v", "--num_validation_samples", type=int, nargs='?', const=True, required=True, help="Number of validation samples to use.")
parser.add_argument("-b", "--num_burn_in_results", type=int, nargs='?', const=True, required=True, help="Number of initial match results generated by random scheduling.")
parser.add_argument("-o", "--output_dir", type=str, nargs='?', const=True, required=True, help="Directory to save output files.")
parser.add_argument("-a", "--append", type=str2bool, nargs='?', const=True, default=False, help="Append previous training data.")
parser.add_argument("-u", "--uncertainty", type=str2bool, nargs='?', const=True, default=False, help="Use uncertainty estimates.")
parser.add_argument("-z", "--noise_magnitude", type=float, nargs='?', const=True, required=True, help="Standard deviation of the noise to add to the oracle scores.")

trimmed_args = sys.argv.copy()
del trimmed_args[0]
parsed_args = parser.parse_args(trimmed_args)

# Load match results from file
nn_input_size = -1
num_match_results = -1
with open(parsed_args.match_results) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    num_match_results = len(list(csv_reader)) - 1
    if parsed_args.debug:
        print('Number of match results = ' + str(num_match_results))
    
with open(parsed_args.match_results) as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count == 0:
            # TODO: Fix this hardcoded 5
            nn_input_size = len(row) - 5
            if parsed_args.debug:
                print('Number of intentions = ' + str(nn_input_size))
            match_results_train = np.zeros([num_match_results, nn_input_size], dtype = float)
            oracle_scores = np.zeros([num_match_results], dtype = float)
            oracle_scores_noisy = np.zeros([num_match_results], dtype = float)
            line_count += 1
        else:
            # TODO: Fix this hardcoded 4
            for i in range(4, 4 + nn_input_size):
                match_results_train[line_count - 1][i - 4] = row[i]
            oracle_scores[line_count - 1] = row[4 + nn_input_size]
            oracle_scores_noisy[line_count - 1] = float(row[4 + nn_input_size]) + np.random.normal(0.0, parsed_args.noise_magnitude)
            line_count += 1
    if parsed_args.debug:
        print('Processed ' + str(line_count) + ' lines.')

used_pairs = {}

# Load previous training samples
if parsed_args.append:

    for file_num in range(0, 2):

        if (file_num == 1) and (parsed_args.num_validation_samples <= 0):
            continue
        
        if file_num == 0:
            filename = "training_data.csv"
        elif file_num == 1:
            filename = "test_data.csv"

        with open(parsed_args.output_dir + "/" + filename) as csv_file:
            csv_reader = csv.reader(csv_file, delimiter=',')
            num_data_rows = len(list(csv_reader)) - 1
            #if parsed_args.debug:
            print('Number of loaded rows = ' + str(num_data_rows))
            
        if file_num == 0:
            loaded_training_data = np.zeros([num_data_rows, 2, nn_input_size], dtype = float)
            loaded_targets = np.zeros([num_data_rows, 1], dtype = float)
            loaded_targets_true = np.zeros([num_data_rows, 1], dtype = float)
            data_arr = loaded_training_data
            lbl_arr = loaded_targets
            lbl_arr_true = loaded_targets_true
        elif file_num == 1:
            loaded_test_data = np.zeros([num_data_rows, 2, nn_input_size], dtype = float)
            loaded_test_targets = np.zeros([num_data_rows, 1], dtype = float)
            loaded_test_targets_true = np.zeros([num_data_rows, 1], dtype = float)
            data_arr = loaded_test_data
            lbl_arr = loaded_test_targets
            lbl_arr_true = loaded_test_targets_true
            
        with open(parsed_args.output_dir + "/" + filename) as csv_file:
            csv_reader = csv.reader(csv_file, delimiter=',')
            line_count = 0
            for row in csv_reader:
                if line_count > 0: # Skip the header
                    # TODO: Fix this hardcoded 4
                    for i in range(0, nn_input_size):
                        data_arr[line_count - 1][0][i] = row[i]
                        data_arr[line_count - 1][1][i] = row[nn_input_size + i]
                        lbl_arr[line_count - 1][0] = row[nn_input_size * 2]
                        lbl_arr_true[line_count - 1][0] = row[nn_input_size * 2 + 1]
                        
                    # Record the pair as being used
                    str_a = match_result_to_str(data_arr[line_count - 1][0])
                    str_b = match_result_to_str(data_arr[line_count - 1][1])
                    if str_a not in used_pairs:
                        used_pairs[str_a] = {}
        
                    if str_b not in used_pairs:
                        used_pairs[str_b] = {}
        
                    used_pairs[str_a][str_b] = True
                    used_pairs[str_b][str_a] = True

                line_count += 1

# Generate new training samples
total_num_samples = parsed_args.num_training_samples + parsed_args.num_validation_samples

if parsed_args.uncertainty:
    oversample_mul = 10
else:
    oversample_mul = 1
    
all_data = np.zeros([total_num_samples * oversample_mul, 2, nn_input_size], dtype = float)
all_targets = np.zeros([total_num_samples * oversample_mul, 1], dtype = float)
all_targets_true = np.zeros([total_num_samples * oversample_mul, 1], dtype = float)
i = 0
while i < total_num_samples * oversample_mul:

    retries = 0
    max_retries = 100
    #while retries < max_retries:
    while True:
        idx_a = randrange(num_match_results)
        idx_b = randrange(num_match_results)
    
        str_a = match_result_to_str(match_results_train[idx_a])
        str_b = match_result_to_str(match_results_train[idx_b])
    
        if str_a not in used_pairs:
            used_pairs[str_a] = {}
        
        if str_b not in used_pairs:
            used_pairs[str_b] = {}
        
        # Accept the sample if retries >= max_retries so that the program doesn't just crash if we request too many samples
        if retries >= max_retries:
            used_pairs[str_a][str_b] = True
            used_pairs[str_b][str_a] = True
            break
        elif str_a == str_b: # Prevent identical match results since the model can't learn anything from these
            retries = retries + 1
        elif (str_b in used_pairs[str_a]) or (str_a in used_pairs[str_b]):
            retries = retries + 1
        else:
            used_pairs[str_a][str_b] = True
            used_pairs[str_b][str_a] = True
            break
            
    #if retries >= max_retries:
    #    print('ERROR: Not enough match results to avoid creating duplicate training data.')
    #    sys.exit(0)
        
    np.copyto(all_data[i][0], match_results_train[idx_a])
    np.copyto(all_data[i][1], match_results_train[idx_b])
    all_targets[i][0] = get_preference(oracle_scores_noisy[idx_a], oracle_scores_noisy[idx_b])
    all_targets_true[i][0] = get_preference(oracle_scores[idx_a], oracle_scores[idx_b])
    i += 1

if parsed_args.uncertainty:
    all_data_tens = torch.from_numpy(all_data).float()
    
    pref_learner_for_selection = PreferenceLearner(-1, nn_input_size, NUM_HIDDEN)
    if parsed_args.append:
        pref_learner_for_selection.load_model(parsed_args.output_dir)
        
    # TODO: Parameterise 'num_fwd_passes'
    top_n_idxs = pref_learner_for_selection.get_uncertainty(all_data_tens, total_num_samples, num_fwd_passes=10)
    
    all_data = np.take(all_data, top_n_idxs, axis=0)
    all_targets = np.take(all_targets, top_n_idxs, axis=0)
    all_targets_true = np.take(all_targets_true, top_n_idxs, axis=0)
    
# Shuffle so that the *most* uncertain examples don't always become training examples (or is it test?)
shuffler = np.random.permutation(len(all_data))
all_data = all_data[shuffler]
all_targets = all_targets[shuffler]
all_targets_true = all_targets_true[shuffler]

training_data_np = all_data[0 : parsed_args.num_training_samples]
targets_np = all_targets[0 : parsed_args.num_training_samples]
targets_true_np = all_targets_true[0 : parsed_args.num_training_samples]

if parsed_args.num_validation_samples > 0:
    test_data_np = all_data[parsed_args.num_training_samples : total_num_samples]
    test_targets_np = all_targets[parsed_args.num_training_samples : total_num_samples]
    test_targets_true_np = all_targets_true[parsed_args.num_training_samples : total_num_samples]
    
if parsed_args.append:

    # Concatenate the loaded data with the new data
    training_data_np = np.concatenate((loaded_training_data, training_data_np), axis=0)
    targets_np = np.concatenate((loaded_targets, targets_np), axis=0)
    targets_true_np = np.concatenate((loaded_targets_true, targets_true_np), axis=0)
    
    if parsed_args.num_validation_samples > 0:
        test_data_np = np.concatenate((loaded_test_data, test_data_np), axis=0)
        test_targets_np = np.concatenate((loaded_test_targets, test_targets_np), axis=0)
        test_targets_true_np = np.concatenate((loaded_test_targets_true, test_targets_true_np), axis=0)
        
# Convert to tensors for training
training_data = torch.from_numpy(training_data_np).float()
targets = torch.from_numpy(targets_np).float()

if parsed_args.num_validation_samples > 0:
    test_data = torch.from_numpy(test_data_np).float()
    test_targets = torch.from_numpy(test_targets_np).float()

# Training
# TODO: Should possibly batch this if we want to use more than 100 or so training samples.
pref_learner = PreferenceLearner(-1, nn_input_size, NUM_HIDDEN)
pref_learner_best = PreferenceLearner(-1, nn_input_size, NUM_HIDDEN)
pref_learner_best.load_state_dict(pref_learner.state_dict())
optimizer = optim.Adam(pref_learner.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1E-8, weight_decay=0.0)
#optimizer = optim.Adam(pref_learner.parameters(), lr=0.0025, betas=(0.9, 0.999), eps=1E-8, weight_decay=0.0001)
#optimizer = optim.SGD(pref_learner.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)
#loss = nn.MSELoss()
loss = nn.BCELoss()

training_iter = 0

least_train_error = float('inf')
while training_iter < 5000:

    predicted_pref = pref_learner.predict_preference(training_data, use_dropout=True) # Set use_dropout=True always because we want the regularisation effect.
    error = loss(predicted_pref, targets)

    optimizer.zero_grad()
    error.backward()
    #print('grad norm = ', pref_learner.get_grad_norm())
    optimizer.step()

    # Evaluate test error in eval mode (in case we're using dropout)
    if parsed_args.num_validation_samples > 0:
        pref_learner.eval()
        predicted_pref_test = pref_learner.predict_preference(test_data, use_dropout=False)
        test_error = loss(predicted_pref_test, test_targets)
    
    pref_learner.train()
        
    if error < least_train_error:
        least_train_error = error
        pref_learner_best.load_state_dict(pref_learner.state_dict())
        
    if parsed_args.debug:
        print('train error = ', error)
        if parsed_args.num_validation_samples > 0:
            print('test error = ', test_error)
        print('least train error = ', least_train_error)
        print('')

    training_iter += 1

if parsed_args.debug:
    print('training done!')
    
if parsed_args.debug and parsed_args.num_validation_samples > 0:

    # TODO: Would be good to try normalising the oracle scores and the model scores to see how they compare post-normalisation.
    #print('test data 0 first player score = ', calculate_score(test_data[0][0].numpy()))
    #print('test data 0 second player score = ', calculate_score(test_data[0][1].numpy()))

    print('test data 0 first player MODEL score = ', pref_learner.forward(test_data[0][0], use_dropout=False))
    print('test data 0 second player MODEL score = ', pref_learner.forward(test_data[0][1], use_dropout=False))
    print('test data 0 first player preferred? ', pref_learner.predict_preference(test_data[0], use_dropout=False))

    test_vals = np.zeros([nn_input_size], dtype = float)
    test_vals[0] = 1.0
    test_vals[1] = 0.0
    test_vals[2] = 1.0
    test_vals[3] = 0.0
    test_vals[4] = 1.0
    test_vals[5] = 0.0
    test_vals[6] = 1.0
    test_vals[7] = 0.0
    test_vals[8] = 1.0
    test_vals[9] = 0.0
    test_vals[10] = 1.0
    test_vals[11] = 0.0
    test_vals = torch.from_numpy(test_vals).float()
    print('test value for reco', pref_learner.forward(test_vals, use_dropout=False))

# Write training and test data
for file_num in range(0, 2): # Change to range(0, 2) to debug the data loading

    if (file_num == 1 or file_num == 3) and (parsed_args.num_validation_samples <= 0):
        continue

    if file_num == 0:
        tmp_file = open(parsed_args.output_dir + "/training_data.csv","w") # Could use "a" for efficiency but prefer to write the whole file for debugging purposes.
    elif file_num == 1:
        tmp_file = open(parsed_args.output_dir + "/test_data.csv","w")
    elif file_num == 2:
        tmp_file = open(parsed_args.output_dir + "/debug_training_data.csv","w")
    elif file_num == 3:
        tmp_file = open(parsed_args.output_dir + "/debug_test_data.csv","w")
            
    # Header
    # Revert the below line if going back to "a" above.
    if True: #not parsed_args.append:
        for j in range(0, 2):
            for k in range(0, nn_input_size):
                tmp_file.write("S" + str(j) + "I" + str(k) + ",")
        tmp_file.write("LBL,LBL_TRUE\n")
    
    if file_num == 0:
        data_arr = training_data_np
        lbl_arr = targets_np
        lbl_arr_true = targets_true_np
    elif file_num == 1:
        data_arr = test_data_np
        lbl_arr = test_targets_np
        lbl_arr_true = test_targets_true_np
    elif file_num == 2:
        data_arr = loaded_training_data
        lbl_arr = loaded_targets
        lbl_arr_true = loaded_targets_true
    elif file_num == 3:
        data_arr = loaded_test_data
        lbl_arr = loaded_test_targets
        lbl_arr_true = loaded_test_targets_true
            
    for i in range(0, len(data_arr)):
        for j in range(0, 2):
            for k in range(0, nn_input_size):
                tmp_file.write(str(data_arr[i][j][k]) + ",")
        tmp_file.write(str(lbl_arr[i][0]) + ",")
        tmp_file.write(str(lbl_arr_true[i][0]))
        tmp_file.write("\n")

tmp_file.close()

# Normalise best model's output prior to saving
match_results_burn_in_tens = torch.from_numpy(match_results_train[0:parsed_args.num_burn_in_results]).float()
pref_learner_best.normalise_output(match_results_burn_in_tens)


# Plot model score versus oracle score
match_results_train_tens = torch.from_numpy(match_results_train).float()
match_results_train_scores = pref_learner_best.forward(match_results_train_tens, False).detach().squeeze().numpy()

df = pd.DataFrame({'Oracle score': oracle_scores, 'Model score (normalised)': match_results_train_scores})
graph = seaborn.scatterplot(x='Oracle score', y='Model score (normalised)', data=df, alpha=1.0, s=10)
#graph = seaborn.regplot(x='Oracle score', y='Model score', data=df)

plt.savefig(parsed_args.output_dir + '/model_score_vs_oracle_score_after_' + str(training_data.shape[0]) + '_samples.png')    


# Save the model with the least test error
pref_learner_best.save_model(parsed_args.output_dir)
#pref_learner.save_model(os.path.dirname(os.path.realpath(__file__)))

# Set below to True to test save / load functionality
if False:
    #pref_learner_best.get_uncertainty(training_data, top_n=10)

    pref_learner_old = PreferenceLearner(-1, nn_input_size, NUM_HIDDEN)
    pref_learner_old.load_model(parsed_args.output_dir)

    test_vals = np.zeros([nn_input_size], dtype = float)
    test_vals[0] = 1.0
    test_vals[1] = 0.0
    test_vals[2] = 1.0
    test_vals[3] = 0.0
    test_vals[4] = 1.0
    test_vals[5] = 0.0
    test_vals[6] = 1.0
    test_vals[7] = 0.0
    test_vals[8] = 1.0
    test_vals[9] = 0.0
    test_vals[10] = 1.0
    test_vals[11] = 0.0
    test_vals = torch.from_numpy(test_vals).float()

    print('test value for reco BEST', pref_learner_best.forward(test_vals, use_dropout=False))
    print('test value for reco ALT', pref_learner_old.forward(test_vals, use_dropout=False))

